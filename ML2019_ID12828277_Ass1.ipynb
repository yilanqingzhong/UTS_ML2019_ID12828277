{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yilanqingzhong/UTS_ML2019_ID12828277/blob/master/ML2019_ID12828277_Ass1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFalIDGgS1oL",
        "colab_type": "text"
      },
      "source": [
        "# Review Report on \"Eigenfaces vs. Fisherfaces: Recognition Using Class Specific Linear Projection\"\n",
        "\n",
        "## **Introduction**\n",
        "\n",
        "The paper \"Eigenfaces vs. Fisherfaces: Recognition Using Class Specific Linear Projection\" by Belhumeur et al. was published several years later when Turk & Pentland first applied Eigenfaces algorithm to face classification for the first time. \n",
        "\n",
        "## **Content**\n",
        "Based on Fisher's linear discriminant, Belhumeur et al. proposed the Fisherface method, trying applying the discriminant to face classification, and producing well-separated classes in a low-dimensional subspace with relatively large changes in lighting direction and facial expression. In this paper, Belhumeur et al. discussed four methods and used them to test samples. Extensive experimental results show that the \"Fisherface\" method is superior to the other three Eigenface techniques.\n",
        "\n",
        "The Correlation method, in general, is to normalize all images to zero mean and unit variance. This process can select the image most correlated to the test image in the learning set. But the disadvantages of correlation method are also obvious. It requires a learning set that densely samples possible continuous lighting conditions, and it requires a lot of computation and storage.\n",
        "\n",
        "Compared to the correlation method, the linear subspace method performs better on recognizing faces, and it does not need a large number of images to represent the variation of each class. However, it still requires three images for each person, and it also requires a lot of computation.\n",
        "\n",
        "For the Eigenface method, Principal Component Analysis(PCA) is used to find the main components of a face. To be specific, the average value of all face images in the training set is calculated first. See Fig. 1. Then, calculate the covariance matrixes for all face images in the training set. After that, the eigenvalue decomposition of the covariance matrixes is carried out to obtain the corresponding eigenvectors, i.e. \"eigenfaces\". See Fig. 2. The eigenvector corresponding to the largest eigenvalue is selected to form the eigensubspace, and the original sample is projected into the subspace, achieving the effect of dimensionality reduction. However, under different illumination conditions, the eigenvectors of the changes caused by illumination are retained in the eigensubspace. Therefore, classes in the projection space may be mixed together.\n",
        "\n",
        "<img src=\"https://github.com/yilanqingzhong/UTS_ML2019_ID12828277/blob/master/1.png?raw=true\" width=\"500\"/>\n",
        "\n",
        "Fig. 1. (a) Face images used as the training set\n",
        "\n",
        "<img src=\"https://github.com/yilanqingzhong/UTS_ML2019_ID12828277/blob/master/2.png?raw=true\" width=\"500\"/>\n",
        "\n",
        "Fig. 1. (b)The average face of training set\n",
        "\n",
        "<img src=\"https://github.com/yilanqingzhong/UTS_ML2019_ID12828277/blob/master/3.png?raw=true\" width=\"500\"/>\n",
        "\n",
        "Fig. 2. Seven of the eigenfaces calculated from the input images of Fig. 1.\n",
        "\n",
        "Linear Discriminant Analysis(LDA) and PCA are very similar. The difference between them is that LDA aims to find the projection direction with the best classification performance, while PCA aims to find the projection direction with the largest variance among samples. The core idea of LDA is to project samples from high dimension into a low-dimensional subspace, which can extract important classification information and conduct dimensionality reduction. By doing so, the samples have the maximal within-class distance and the minimal between-class distance in the new subspace. Therefore, the original sample has the best separability in this new low-dimensional space. The formula is shown below:\n",
        "\n",
        "<img src=\"https://github.com/yilanqingzhong/UTS_ML2019_ID12828277/blob/master/4.png?raw=true\" width=\"200\"/>\n",
        "\n",
        "In practice, however, within-class scatter matrix is always singular. Because the image dimensions are relatively high, it is difficult or impossible to find enough training samples to ensure the reversibility of within-class scatter matrix. Consequently, Fisherface method was proposed in this case. Use PCA method first. Then, project the images into a lower-dimensional space, and the non-singular within-class scatter matrix was obtained, so as to obtain the best subspace.\n",
        "\n",
        "## **Innovation**\n",
        "In the late 20th century, many face recognition algorithms were proposed. Belhumeur et al. argued that face recognition techniques had made great progress with small changes in light, facial expression, and pose. However, in fact, accurate face recognition is difficult to achieve in more extreme situations. Therefore, in this context, Belhumeur et al. Proposed a constructive and innovative technique, i.e., Fisherface, which is almost unaffected by light, facial expression, and the wearing of glasses. \n",
        "\n",
        "Eigenfaces is a way to use PCA to retain as much important information as possible from the original data after they are projected and reduce redundant information. Based on using PCA to conduct dimensionality reduction, Fisherface also uses LDA to ensure that the face images of different images of the same person gather together after projection, and the face images of different people are separated by a large distance after projection, achieving a good classification effect. With the support of existing numerical algorithms at that time, it is not a difficult problem to solve the eigenvalues and eigenvectors of matrixes. The proposed Fisherface method greatly reduced the error rate of face recognition in the experiment.\n",
        "\n",
        "Another novelty of this paper is that Belhumeur et al. not only used the database of methodically varied illumination from Harvard Robotics Laboratory, but also another database created by the authors at the Yale Center for Computational Vision and Control was used, and tests were designed to determine the comparison among these face recognition under different conditions.\n",
        "\n",
        "At the end of the paper, Belhumeur et al. also put forward some thoughts and suggestions based on the experimental results, pointing out the direction for face recognition in the future. For example, whether the Fisherface method can also do well in larger databases. New detection methods are required to support the algorithms in the paper to deal with extreme conditions.\n",
        "\n",
        "## **Technical quality**\n",
        "The technical quality of this paper is relatively high, which benefits from the rigour of the content of this paper. Belhumeur et al. cited a lot of literature as theoretical support, and adopted data from two databases, supporting their theory through comparison between four methods. The authors also consider the limitations of these techniques. For example, the authors pointed out that faces are not truly Lambertian surfaces. Because of self-shading, images deviate from linear subspaces. Besides, the performance of these algorithms is based on the Harvard and Yale databases, and the authors make no claim about how these algorithms will perform in larger databases.\n",
        "\n",
        "Belhumeur et al. conducted experiments on two different databases using extrapolation, extrapolation, and leaving-one-out strategy, verifying their speculation. In the test for Eigenfaces, authors found that the first three principal components were mainly caused by lighting variation. The linear subspace method, as expected by the authors, had problems dealing with changes in facial expression. Because of the change of facial expression, the image is no longer in linear subspace. Before comparing the linear subspace method with the Fisherface method and the Eigenface method, the authors conducted an experiment to determine the number of principal components that produced the lowest error rate.\n",
        "\n",
        "To sum up, this paper can be replicated. In other words, Belhumeur et al. clearly described the theories and experimental procedures. However, this paper can be improved, mainly for beginners. Beginners may not have a good understanding of how pixels in a face image are viewed as coordinates in high-dimensional space and how these pixels are arranged as vectors in real experiments.\n",
        "\n",
        "Application and X-factor\n",
        "From my perspective, the Fisherface method and the Eigenface method can serve as a basis in the field of face recognition or other linear classification problems. The core ideas of these two methods provide guidance for more advanced algorithms, but they cannot be applied directly in the field of face recognition because of the lack of generalization. \n",
        "\n",
        "It must be admitted that, because of self-shading, faces are not truly Lambertian surfaces. Therefore, the projection of face features into the linear subspace will result in the loss of some valid information, and thus the face recognition cannot be error-free. So, other nonlinear dimensionality reduction algorithms such as manifold learning or kernel technology may get better results than linear dimensionality reduction techniques. Besides, the experiments in this paper are all based on the nearest neighbor classifier. Maturer classifier schemes can be adopted, such as neural network, VSM, Bayes, and so forth. Artificial design features can also be replaced by deep learning. \n",
        "\n",
        "There are also several interesting points about the experiments. Belhumeur et al. found that the overall shape of a face is also an important feature for face recognition when using the Fichserface method for an entire face. Another example is that by removing the first three principal components, the accuracy of Eigenfaces increases with the dimension of the Eigenspace and stabilizes with 45 principal components.\n",
        "\n",
        "\n",
        "## Presentation\n",
        "Overall, this paper is clear in structure, readable, logical, and closely related to the topic. Specifically, the abstract section clearly summarizes the main content of this paper. In the main part, Belhumeur et al. appropriately present the background, theory, experimental process, and conclusion. What are quoted in the paper are reliable. The authors also make clear conclusions and explanations of the experimental results and summarized the key points of the paper. The formulas that appear in the paper are explained in detail by the authors, which helps the reader understand the meaning of the symbols. Belhumeur et al. also used a large number of figures and tables, which also contributed to readers' intuitive understanding. Besides, there are detailed annotations below all figures. For example, the figure below is the comparison between LDA and PCA in the two-dimensional case, which enables readers to understand the differences between LDA and PCA quickly and easily.\n",
        "\n",
        "<img src=\"https://github.com/yilanqingzhong/UTS_ML2019_ID12828277/blob/master/5.png?raw=true\" width=\"500\"/>\n",
        "\n",
        "Fig. 3. An example in the paper\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Reference\n",
        "Belhumeur, P., Hespanha, J., Kriegman, D. 1997, 'Eigenfaces vs. Fisherfaces: Recognition Using Class Specific Linear Projection', *IEEE Transactions on Pattern Analysis and Machine Intelligence*, vol. 19, no. 7, pp. 711-720.\n",
        "\n",
        "Turk. M. A. & Pentland, A. P. 1991, ‘Face recognition using eigenfaces’, *IEEE Computer Society Conference on Computer Vision and Pattern Recognition*, IEEE, Piscataway, NJ, pp. 586-591.\n",
        "\n",
        "Fisher, R. A. 1936, ‘THE USE OF MULTIPLE MEASUREMENTS IN TAXONOMIC PROBLEMS’, *Annals of Eugenics*, vol. 7, no. 2, pp.179-188."
      ]
    }
  ]
}